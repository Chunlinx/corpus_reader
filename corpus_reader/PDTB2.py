# -*- coding: utf-8 -*-

from collections import defaultdict, OrderedDict
import json
import os
import sys

attribute_names = [
    "Relation",
    "Section",
    "FileNumber",
    "Connective_SpanList",
    "Connective_GornList",
    "Connective_RawText",
    # "Connective_Trees",
    "Connective_StringPosition",
    "SentenceNumber",
    "ConnHead",
    "Conn1",
    "Conn2",
    "ConnHeadSemClass1",
    "ConnHeadSemClass2",
    "Conn2SemClass1",
    "Conn2SemClass2",
    "Attribution_Source",
    "Attribution_Type",
    "Attribution_Polarity",
    "Attribution_Determinacy",
    "Attribution_SpanList",
    "Attribution_GornList",
    "Attribution_RawText",
    "Arg1_SpanList",
    "Arg1_GornList",
    "Arg1_RawText",
    # "Arg1_Trees",
    "Arg1_Attribution_Source",
    "Arg1_Attribution_Type",
    "Arg1_Attribution_Polarity",
    "Arg1_Attribution_Determinacy",
    "Arg1_Attribution_SpanList",
    "Arg1_Attribution_GornList",
    "Arg1_Attribution_RawText",
    # "Arg1_Attribution_Trees",
    "Arg2_SpanList",
    "Arg2_GornList",
    "Arg2_RawText",
    # "Arg2_Trees",
    "Arg2_Attribution_Source",
    "Arg2_Attribution_Type",
    "Arg2_Attribution_Polarity",
    "Arg2_Attribution_Determinacy",
    "Arg2_Attribution_SpanList",
    "Arg2_Attribution_GornList",
    "Arg2_Attribution_RawText",
    "Sup1_SpanList",
    "Sup1_GornList",
    "Sup1_RawText",
    # "Sup1_Trees",
    "Sup2_SpanList",
    "Sup2_GornList",
    "Sup2_RawText",
    # "Sup2_Trees",
]
assert len(attribute_names) == 48

# Following the paper of Braud & Denis (2015).
sections_for_eachsplit = {
    "train": ["%02d" % i for i in xrange(2, 21)],
    "test": ["21", "22"],
    "val": ["00", "01", "23", "24"],
}

class PDTB2(object):
    """
    Penn Discourse Treebank 2.0
    """

    def __init__(self, path):
        """
        the ``path'' is assumed to be the root directory for *.pipe files
        *.pipe files can be generated by the script ``convert.pl'' provided by the dataset authors.
        """
        def load_split(path, relation, sections):
            data = []
            dirnames = os.listdir(path)
            dirnames = [n for n in dirnames if n in sections]
            for dirname in dirnames:
                filenames = os.listdir(os.path.join(path, dirname))
                for filename in filenames:
                    for line in open(os.path.join(path, dirname, filename)):
                        line = line.decode("latin-1").strip().split("|")
                        if len(line) != len(attribute_names):
                            print("Error! # of attributes is mismatch: %d(actual) != %d(expected)" \
                                    % (len(line), len(attribute_names)) )
                            sys.exit(-1)
                        if (relation != "all") and (relation is not None):
                            if line[0] != relation:
                                continue
                        datum = OrderedDict()
                        for key, val in zip(attribute_names, line):
                            datum[key.decode("utf-8")] = val
                        data.append(datum)
            return data
        self.dataset = {}
        for relation in ["all", "Explicit", "Implicit", "AltLex", "EntRel", "NoRel"]:
            for split in ["train", "val", "test"]:
                self.dataset[relation + "." + split] = load_split(path, relation=relation, sections=sections_for_eachsplit[split])
        
        # check
        d = defaultdict(int)
        print("Total instances: %d" % (len(self.dataset["all.train"]) + len(self.dataset["all.val"]) + len(self.dataset["all.test"])))
        for split in ["train", "val", "test"]:
            for datum in self.dataset["all." + split]:
                d[datum["Relation"]] += 1
        for key, val in d.iteritems():
            print("%s: %d" % (key, val))
        
    def get(self, relation, split):
        return self.dataset[relation + "." + split]

    def dump(self, path_dir):
        # NOTE: Just do ``data = json.load(open(path))'' when loading.
        for relation in ["all", "Explicit", "Implicit", "AltLex", "EntRel", "NoRel"]:
            for split in ["train", "val", "test"]:
                data = self.dataset[relation + "." + split]

                with open(os.path.join(path_dir, "pdtb2.0.%s.%s.json" % (relation, split)), "w") as f:
                    json.dump(data, f, indent=2)

                f_arg1 = open(os.path.join(path_dir, "pdtb2.0.%s.%s.Arg1_RawText.txt" % \
                        (relation, split)), "w")
                f_arg2 = open(os.path.join(path_dir, "pdtb2.0.%s.%s.Arg2_RawText.txt" % \
                        (relation, split)), "w")
                f_connraw = open(os.path.join(path_dir, "pdtb2.0.%s.%s.Connective_RawText.txt" % \
                        (relation, split)), "w")
                f_conninf = open(os.path.join(path_dir, "pdtb2.0.%s.%s.Conn1.txt" % \
                        (relation, split)), "w")
                f_conncls = open(os.path.join(path_dir, "pdtb2.0.%s.%s.ConnHeadSemClass1.txt" % \
                        (relation, split)), "w")
                for datum in data:
                    f_arg1.write("%s\n" % datum[u"Arg1_RawText"].encode("utf-8"))
                    f_arg2.write("%s\n" % datum[u"Arg2_RawText"].encode("utf-8"))
                    f_connraw.write("%s\n" % datum[u"Connective_RawText"].encode("utf-8"))
                    f_conninf.write("%s\n" % datum[u"Conn1"].encode("utf-8"))
                    f_conncls.write("%s\n" % datum[u"ConnHeadSemClass1"].encode("utf-8"))
                f_arg1.flush()
                f_arg2.flush()
                f_connraw.flush()
                f_conninf.flush()
                f_conncls.flush()
                f_arg1.close()
                f_arg2.close()
                f_connraw.close()
                f_conninf.close()
                f_conncls.close()

