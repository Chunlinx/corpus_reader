# -*- coding: utf-8 -*-

from collections import defaultdict, OrderedDict
import json
import os
import sys

attribute_names = [
    "Relation",
    "Section",
    "FileNumber",
    "Connective_SpanList",
    "Connective_GornList",
    "Connective_RawText",
    # "Connective_Trees",
    "Connective_StringPosition",
    "SentenceNumber",
    "ConnHead",
    "Conn1",
    "Conn2",
    "ConnHeadSemClass1",
    "ConnHeadSemClass2",
    "Conn2SemClass1",
    "Conn2SemClass2",
    "Attribution_Source",
    "Attribution_Type",
    "Attribution_Polarity",
    "Attribution_Determinacy",
    "Attribution_SpanList",
    "Attribution_GornList",
    "Attribution_RawText",
    "Arg1_SpanList",
    "Arg1_GornList",
    "Arg1_RawText",
    # "Arg1_Trees",
    "Arg1_Attribution_Source",
    "Arg1_Attribution_Type",
    "Arg1_Attribution_Polarity",
    "Arg1_Attribution_Determinacy",
    "Arg1_Attribution_SpanList",
    "Arg1_Attribution_GornList",
    "Arg1_Attribution_RawText",
    # "Arg1_Attribution_Trees",
    "Arg2_SpanList",
    "Arg2_GornList",
    "Arg2_RawText",
    # "Arg2_Trees",
    "Arg2_Attribution_Source",
    "Arg2_Attribution_Type",
    "Arg2_Attribution_Polarity",
    "Arg2_Attribution_Determinacy",
    "Arg2_Attribution_SpanList",
    "Arg2_Attribution_GornList",
    "Arg2_Attribution_RawText",
    "Sup1_SpanList",
    "Sup1_GornList",
    "Sup1_RawText",
    # "Sup1_Trees",
    "Sup2_SpanList",
    "Sup2_GornList",
    "Sup2_RawText",
    # "Sup2_Trees",
]
assert len(attribute_names) == 48

# Following the paper of Braud & Denis (2015).
sections_for_eachsplit = {
    "train": ["%02d" % i for i in xrange(2, 21)],
    "val": ["00", "01", "23", "24"],
    "test": ["21", "22"]
}

class PDTB2(object):
    """
    Penn Discourse Treebank 2.0
    """

    def __init__(self, path):
        """
        the ``path'' is assumed to be the root directory for *.pipe files
        *.pipe files can be generated by the script ``convert.pl'' provided by the dataset authors.
        """
        def load_split(path, sections):
            dataset_split = []
            dirnames = os.listdir(path)
            dirnames = [n for n in dirnames if n in sections]
            for dirname in dirnames:
                filenames = os.listdir(os.path.join(path, dirname))
                for filename in filenames:
                    for line in open(os.path.join(path, dirname, filename)):
                        line = line.decode("latin-1").strip().split("|")
                        if len(line) != len(attribute_names):
                            print("Error! # of attributes is mismatch: %d(actual) != %d(expected)" \
                                    % (len(line), len(attribute_names)) )
                            sys.exit(-1)
                        datum = OrderedDict()
                        for key, val in zip(attribute_names, line):
                            datum[key.decode("utf-8")] = val
                        dataset_split.append(datum)
            return dataset_split
        self.dataset = {}
        for split in ["train", "val", "test"]:
            self.dataset[split] = load_split(path, sections=sections_for_eachsplit[split])
        
        # check
        d = defaultdict(int)
        print("Total instances: %d" % (len(self.dataset["train"]) + len(self.dataset["val"]) + len(self.dataset["test"])))
        for split in ["train", "val", "test"]:
            for datum in self.dataset[split]:
                d[datum["Relation"]] += 1
        for key, val in d.iteritems():
            print("%s: %d" % (key, val))
        
    def get(self, split):
        return self.dataset[split]

    def dump(self, path_dir):
        # NOTE: Just do ``data = json.load(open(path))'' when loading.
        for split in ["train", "val", "test"]:
            with open(os.path.join(path_dir, "pdtb2.0.%s.json" % split), "w") as f:
                json.dump(self.dataset[split], f, indent=2)

